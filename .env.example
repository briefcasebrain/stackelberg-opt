# Stackelberg-Opt Environment Variables
# Copy this file to .env and configure with your settings

# Language Model Configuration
# The model name to use for prompt mutations and optimizations
STACKELBERG_MODEL=gpt-3.5-turbo

# Temperature setting for language model generation (0.0 - 1.0)
# Lower values = more focused/deterministic, Higher values = more creative/random
STACKELBERG_TEMPERATURE=0.7

# Optional: API Key for your language model provider
# LITELLM_API_KEY=your-api-key-here

# Optional: For OpenAI models
# OPENAI_API_KEY=your-openai-api-key

# Optional: For Anthropic models
# ANTHROPIC_API_KEY=your-anthropic-api-key

# Optional: For Azure OpenAI
# AZURE_API_KEY=your-azure-api-key
# AZURE_API_BASE=https://your-resource.openai.azure.com/
# AZURE_API_VERSION=2023-05-15

# Optional: For custom API endpoints
# LITELLM_API_BASE=https://your-custom-endpoint.com

# Optional: Set logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
# STACKELBERG_LOG_LEVEL=INFO

# Optional: Cache directory for storing API responses
# STACKELBERG_CACHE_DIR=.cache

# Optional: Enable/disable caching (true/false)
# STACKELBERG_CACHE_ENABLED=true