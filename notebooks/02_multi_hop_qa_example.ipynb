{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Hop Question Answering with stackelberg-opt\n",
    "\n",
    "This notebook demonstrates how to build and optimize a multi-hop question answering system using Stackelberg game theory.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Multi-hop QA requires multiple reasoning steps to answer complex questions. We'll create a system where:\n",
    "- **Leader module**: Generates initial search queries\n",
    "- **Follower modules**: Retrieve context and generate follow-up queries\n",
    "- **Independent module**: Synthesizes the final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from stackelberg_opt import (\n",
    "    Module, ModuleType, SystemCandidate, ExecutionTrace,\n",
    "    StackelbergOptimizer, OptimizerConfig\n",
    ")\n",
    "from stackelberg_opt.components import (\n",
    "    StackelbergFeedbackExtractor,\n",
    "    DependencyAnalyzer\n",
    ")\n",
    "from stackelberg_opt.utils import OptimizationVisualizer\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the Multi-Hop QA System\n",
    "\n",
    "We'll create a system with strategic interactions between modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_hop_qa_modules() -> Dict[str, Module]:\n",
    "    \"\"\"Create modules for multi-hop question answering.\"\"\"\n",
    "    \n",
    "    modules = {\n",
    "        # Leader: Initial query generation\n",
    "        \"query_generator\": Module(\n",
    "            name=\"query_generator\",\n",
    "            prompt=\"\"\"Given a complex question that requires multiple steps to answer,\n",
    "generate the FIRST search query to find relevant information.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Consider what foundational information is needed first.\n",
    "Output only the search query, nothing else.\n",
    "\n",
    "Search query:\"\"\",\n",
    "            module_type=ModuleType.LEADER,\n",
    "            dependencies=[]\n",
    "        ),\n",
    "        \n",
    "        # Follower 1: Context retrieval\n",
    "        \"context_retriever\": Module(\n",
    "            name=\"context_retriever\",\n",
    "            prompt=\"\"\"Retrieve and summarize information for this search query.\n",
    "\n",
    "Query: {query}\n",
    "Original Question: {question}\n",
    "\n",
    "Provide a focused summary of information that would be found for this query.\n",
    "Focus on facts relevant to answering the original question.\n",
    "\n",
    "Retrieved information:\"\"\",\n",
    "            module_type=ModuleType.FOLLOWER,\n",
    "            dependencies=[\"query_generator\"]\n",
    "        ),\n",
    "        \n",
    "        # Follower 2: Follow-up query generation\n",
    "        \"followup_generator\": Module(\n",
    "            name=\"followup_generator\",\n",
    "            prompt=\"\"\"Based on the initial information, determine if more data is needed.\n",
    "\n",
    "Question: {question}\n",
    "Initial Query: {initial_query}\n",
    "Information Found: {context}\n",
    "\n",
    "If the information is sufficient to answer the question, output: NONE\n",
    "Otherwise, generate ONE follow-up search query for missing information.\n",
    "\n",
    "Follow-up query:\"\"\",\n",
    "            module_type=ModuleType.FOLLOWER,\n",
    "            dependencies=[\"query_generator\", \"context_retriever\"]\n",
    "        ),\n",
    "        \n",
    "        # Follower 3: Additional context retrieval\n",
    "        \"followup_retriever\": Module(\n",
    "            name=\"followup_retriever\",\n",
    "            prompt=\"\"\"Retrieve information for the follow-up query.\n",
    "\n",
    "Follow-up Query: {followup_query}\n",
    "Previous Context: {previous_context}\n",
    "Original Question: {question}\n",
    "\n",
    "If the query is 'NONE', output: 'No additional information needed.'\n",
    "Otherwise, provide relevant information for the follow-up query.\n",
    "\n",
    "Additional information:\"\"\",\n",
    "            module_type=ModuleType.FOLLOWER,\n",
    "            dependencies=[\"followup_generator\", \"context_retriever\"]\n",
    "        ),\n",
    "        \n",
    "        # Independent: Answer synthesis\n",
    "        \"answer_synthesizer\": Module(\n",
    "            name=\"answer_synthesizer\",\n",
    "            prompt=\"\"\"Synthesize a comprehensive answer from all gathered information.\n",
    "\n",
    "Question: {question}\n",
    "All Information:\n",
    "{all_information}\n",
    "\n",
    "Provide a clear, accurate, and complete answer that addresses all aspects of the question.\n",
    "Be concise but thorough.\n",
    "\n",
    "Answer:\"\"\",\n",
    "            module_type=ModuleType.INDEPENDENT,\n",
    "            dependencies=[\"context_retriever\", \"followup_retriever\"]\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return modules\n",
    "\n",
    "# Create the modules\n",
    "qa_modules = create_multi_hop_qa_modules()\n",
    "\n",
    "print(f\"Created {len(qa_modules)} modules for multi-hop QA:\")\n",
    "for name, module in qa_modules.items():\n",
    "    deps = f\" (depends on: {', '.join(module.dependencies)})\" if module.dependencies else \"\"\n",
    "    print(f\"  - {name} ({module.module_type.value}){deps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyze Module Dependencies\n",
    "\n",
    "Let's visualize the dependency structure of our system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dependencies\n",
    "analyzer = DependencyAnalyzer()\n",
    "dep_analysis = analyzer.analyze_dependencies(qa_modules)\n",
    "\n",
    "print(\"Dependency Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Is DAG (Directed Acyclic Graph): {dep_analysis['properties']['is_dag']}\")\n",
    "print(f\"Has cycles: {dep_analysis['properties']['has_cycles']}\")\n",
    "print(f\"Max depth: {dep_analysis['properties']['max_depth']}\")\n",
    "\n",
    "print(\"\\nTopological order:\")\n",
    "for i, module in enumerate(dep_analysis['properties']['topological_order']):\n",
    "    print(f\"  {i+1}. {module}\")\n",
    "\n",
    "# Visualize with a simple text diagram\n",
    "print(\"\\nDependency Graph:\")\n",
    "print(\"query_generator (LEADER)\")\n",
    "print(\"    ├── context_retriever (FOLLOWER)\")\n",
    "print(\"    │   ├── followup_generator (FOLLOWER)\")\n",
    "print(\"    │   │   └── followup_retriever (FOLLOWER)\")\n",
    "print(\"    │   └── answer_synthesizer (INDEPENDENT)\")\n",
    "print(\"    └── followup_generator (connection)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Training Data\n",
    "\n",
    "Let's create diverse multi-hop questions for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data with multi-hop questions\n",
    "train_data = [\n",
    "    (\n",
    "        \"What impact did the invention of the printing press have on the Protestant Reformation?\",\n",
    "        \"The printing press, invented by Gutenberg around 1440, dramatically accelerated the Protestant Reformation by enabling mass production of Luther's 95 Theses and Bible translations, spreading reformist ideas rapidly across Europe and undermining the Catholic Church's monopoly on religious texts.\"\n",
    "    ),\n",
    "    (\n",
    "        \"How do coral reefs protect coastal communities from climate change effects?\",\n",
    "        \"Coral reefs act as natural barriers that reduce wave energy by up to 97%, protecting coastal communities from storm surges and erosion intensified by climate change, while also supporting fish populations that provide food security for millions of people.\"\n",
    "    ),\n",
    "    (\n",
    "        \"What role did the Silk Road play in the spread of the Black Death to Europe?\",\n",
    "        \"The Silk Road served as the primary transmission route for the Black Death from Central Asia to Europe in the 1340s, as infected fleas on rats traveled with merchant caravans, reaching European ports through trade ships and causing the pandemic that killed one-third of Europe's population.\"\n",
    "    ),\n",
    "    (\n",
    "        \"How does quantum entanglement enable quantum computing to outperform classical computers?\",\n",
    "        \"Quantum entanglement allows quantum computers to process information exponentially faster by enabling qubits to exist in superposition and be correlated across distances, allowing parallel processing of multiple calculations simultaneously, unlike classical bits that must be processed sequentially.\"\n",
    "    ),\n",
    "    (\n",
    "        \"What factors led to the fall of the Mayan civilization and are there parallels today?\",\n",
    "        \"The Mayan civilization collapsed due to a combination of severe drought, deforestation, overpopulation, and political instability around 900 CE, paralleling modern concerns about climate change, resource depletion, and social inequality that threaten contemporary societies.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"Created {len(train_data)} multi-hop training examples\\n\")\n",
    "print(\"Sample questions:\")\n",
    "for i, (question, _) in enumerate(train_data[:3]):\n",
    "    print(f\"{i+1}. {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement Task Executor\n",
    "\n",
    "Create a sophisticated task executor that simulates the multi-hop QA process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHopQAExecutor:\n",
    "    \"\"\"Simulated executor for multi-hop question answering.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.execution_count = 0\n",
    "        \n",
    "    async def __call__(self, modules: Dict[str, Module], question: str) -> Tuple[str, ExecutionTrace]:\n",
    "        \"\"\"Execute the multi-hop QA pipeline.\"\"\"\n",
    "        import time\n",
    "        import random\n",
    "        \n",
    "        trace = ExecutionTrace()\n",
    "        trace.execution_order = []\n",
    "        trace.module_outputs = {}\n",
    "        trace.module_timings = {}\n",
    "        trace.intermediate_scores = {}\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Generate initial query (Leader)\n",
    "            start_time = time.time()\n",
    "            initial_query = self._generate_query(question)\n",
    "            trace.execution_order.append(\"query_generator\")\n",
    "            trace.module_outputs[\"query_generator\"] = initial_query\n",
    "            trace.module_timings[\"query_generator\"] = time.time() - start_time\n",
    "            trace.intermediate_scores[\"query_generator\"] = 0.7 + random.random() * 0.3\n",
    "            \n",
    "            # Step 2: Retrieve initial context (Follower)\n",
    "            start_time = time.time()\n",
    "            context = self._retrieve_context(initial_query, question)\n",
    "            trace.execution_order.append(\"context_retriever\")\n",
    "            trace.module_outputs[\"context_retriever\"] = context\n",
    "            trace.module_timings[\"context_retriever\"] = time.time() - start_time\n",
    "            trace.intermediate_scores[\"context_retriever\"] = 0.6 + random.random() * 0.3\n",
    "            \n",
    "            # Step 3: Generate follow-up query (Follower)\n",
    "            start_time = time.time()\n",
    "            followup = self._generate_followup(question, initial_query, context)\n",
    "            trace.execution_order.append(\"followup_generator\")\n",
    "            trace.module_outputs[\"followup_generator\"] = followup\n",
    "            trace.module_timings[\"followup_generator\"] = time.time() - start_time\n",
    "            trace.intermediate_scores[\"followup_generator\"] = 0.65 + random.random() * 0.3\n",
    "            \n",
    "            # Step 4: Retrieve follow-up context (Follower)\n",
    "            start_time = time.time()\n",
    "            followup_context = self._retrieve_followup(followup, context, question)\n",
    "            trace.execution_order.append(\"followup_retriever\")\n",
    "            trace.module_outputs[\"followup_retriever\"] = followup_context\n",
    "            trace.module_timings[\"followup_retriever\"] = time.time() - start_time\n",
    "            trace.intermediate_scores[\"followup_retriever\"] = 0.6 + random.random() * 0.3\n",
    "            \n",
    "            # Step 5: Synthesize answer (Independent)\n",
    "            start_time = time.time()\n",
    "            all_info = f\"Initial search: {initial_query}\\n{context}\\n\\nFollow-up: {followup}\\n{followup_context}\"\n",
    "            answer = self._synthesize_answer(question, all_info)\n",
    "            trace.execution_order.append(\"answer_synthesizer\")\n",
    "            trace.module_outputs[\"answer_synthesizer\"] = answer\n",
    "            trace.module_timings[\"answer_synthesizer\"] = time.time() - start_time\n",
    "            trace.intermediate_scores[\"answer_synthesizer\"] = 0.75 + random.random() * 0.25\n",
    "            \n",
    "            trace.success = True\n",
    "            trace.final_score = np.mean(list(trace.intermediate_scores.values()))\n",
    "            \n",
    "            self.execution_count += 1\n",
    "            return answer, trace\n",
    "            \n",
    "        except Exception as e:\n",
    "            trace.success = False\n",
    "            trace.error = str(e)\n",
    "            trace.final_score = 0.0\n",
    "            return \"Error in execution\", trace\n",
    "    \n",
    "    def _generate_query(self, question: str) -> str:\n",
    "        \"\"\"Simulate initial query generation.\"\"\"\n",
    "        # Extract key terms from question\n",
    "        key_terms = [word for word in question.split() if len(word) > 4][:3]\n",
    "        return f\"{' '.join(key_terms)} definition history\"\n",
    "    \n",
    "    def _retrieve_context(self, query: str, question: str) -> str:\n",
    "        \"\"\"Simulate context retrieval.\"\"\"\n",
    "        return f\"Information about {query}: Historical background and key concepts relevant to the question.\"\n",
    "    \n",
    "    def _generate_followup(self, question: str, initial_query: str, context: str) -> str:\n",
    "        \"\"\"Simulate follow-up query generation.\"\"\"\n",
    "        if random.random() > 0.3:\n",
    "            return f\"specific impacts and connections related to {initial_query}\"\n",
    "        return \"NONE\"\n",
    "    \n",
    "    def _retrieve_followup(self, followup: str, context: str, question: str) -> str:\n",
    "        \"\"\"Simulate follow-up retrieval.\"\"\"\n",
    "        if followup == \"NONE\":\n",
    "            return \"No additional information needed.\"\n",
    "        return f\"Additional details about {followup}: Specific examples and evidence.\"\n",
    "    \n",
    "    def _synthesize_answer(self, question: str, all_info: str) -> str:\n",
    "        \"\"\"Simulate answer synthesis.\"\"\"\n",
    "        return f\"Based on the gathered information, here is a comprehensive answer to '{question[:50]}...'\"\n",
    "\n",
    "# Create executor instance\n",
    "qa_executor = MultiHopQAExecutor()\n",
    "print(\"Multi-hop QA executor created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Optimization\n",
    "\n",
    "Now let's optimize the multi-hop QA system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure optimizer for multi-hop QA\n",
    "qa_config = OptimizerConfig(\n",
    "    budget=30,  # More budget for complex system\n",
    "    population_size=8,\n",
    "    mutation_rate=0.75,\n",
    "    crossover_rate=0.25,\n",
    "    performance_weight=0.4,\n",
    "    equilibrium_weight=0.35,  # Important for leader-follower dynamics\n",
    "    stability_weight=0.25,\n",
    "    enable_caching=True,\n",
    "    enable_visualization=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Optimizer configuration for multi-hop QA:\")\n",
    "print(json.dumps({\n",
    "    'budget': qa_config.budget,\n",
    "    'population_size': qa_config.population_size,\n",
    "    'weights': {\n",
    "        'performance': qa_config.performance_weight,\n",
    "        'equilibrium': qa_config.equilibrium_weight,\n",
    "        'stability': qa_config.stability_weight\n",
    "    }\n",
    "}, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run optimizer\n",
    "qa_optimizer = StackelbergOptimizer(\n",
    "    system_modules=qa_modules,\n",
    "    train_data=train_data,\n",
    "    task_executor=qa_executor,\n",
    "    config=qa_config\n",
    ")\n",
    "\n",
    "print(\"Starting multi-hop QA optimization...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run optimization\n",
    "best_qa_candidate = await qa_optimizer.optimize_async()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Optimization Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best candidate ID: {best_qa_candidate.candidate_id}\")\n",
    "print(f\"Generation: {best_qa_candidate.generation}\")\n",
    "print(f\"Average score: {best_qa_candidate.get_average_score():.3f}\")\n",
    "print(f\"Equilibrium value: {best_qa_candidate.equilibrium_value:.3f}\")\n",
    "print(f\"Stability score: {best_qa_candidate.stability_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Optimization Results\n",
    "\n",
    "Let's examine how the optimization improved the system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs optimized prompts\n",
    "print(\"Prompt Evolution Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for module_name in qa_modules.keys():\n",
    "    original_prompt = qa_modules[module_name].prompt\n",
    "    optimized_prompt = best_qa_candidate.modules[module_name].prompt\n",
    "    \n",
    "    if original_prompt != optimized_prompt:\n",
    "        print(f\"\\n{module_name}:\")\n",
    "        print(f\"  Changed: YES\")\n",
    "        print(f\"  Original length: {len(original_prompt)} chars\")\n",
    "        print(f\"  Optimized length: {len(optimized_prompt)} chars\")\n",
    "        print(f\"  Length change: {len(optimized_prompt) - len(original_prompt):+d} chars\")\n",
    "    else:\n",
    "        print(f\"\\n{module_name}: No changes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze module performance\n",
    "if best_qa_candidate.traces:\n",
    "    print(\"Module Performance Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    module_scores = {module: [] for module in qa_modules.keys()}\n",
    "    \n",
    "    for trace in best_qa_candidate.traces.values():\n",
    "        for module, score in trace.intermediate_scores.items():\n",
    "            module_scores[module].append(score)\n",
    "    \n",
    "    for module, scores in module_scores.items():\n",
    "        if scores:\n",
    "            avg_score = np.mean(scores)\n",
    "            std_score = np.std(scores)\n",
    "            module_type = qa_modules[module].module_type.value\n",
    "            print(f\"\\n{module} ({module_type}):\")\n",
    "            print(f\"  Average score: {avg_score:.3f}\")\n",
    "            print(f\"  Std deviation: {std_score:.3f}\")\n",
    "            print(f\"  Score range: [{min(scores):.3f}, {max(scores):.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimization progress\n",
    "visualizer = OptimizationVisualizer()\n",
    "\n",
    "if hasattr(qa_optimizer, 'population_manager') and qa_optimizer.population_manager.generation_stats:\n",
    "    # Create custom visualization\n",
    "    stats = qa_optimizer.population_manager.generation_stats\n",
    "    generations = sorted(stats.keys())\n",
    "    \n",
    "    avg_fitness = [stats[g].get('avg_fitness', 0) for g in generations]\n",
    "    best_fitness = [stats[g].get('best_fitness', 0) for g in generations]\n",
    "    diversity = [stats[g].get('diversity', 0) for g in generations]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    \n",
    "    # Plot fitness\n",
    "    ax1.plot(generations, avg_fitness, 'b-', label='Average Fitness', linewidth=2)\n",
    "    ax1.plot(generations, best_fitness, 'r-', label='Best Fitness', linewidth=2)\n",
    "    ax1.set_xlabel('Generation')\n",
    "    ax1.set_ylabel('Fitness Score')\n",
    "    ax1.set_title('Multi-Hop QA Optimization Progress')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot diversity\n",
    "    ax2.plot(generations, diversity, 'g-', label='Population Diversity', linewidth=2)\n",
    "    ax2.set_xlabel('Generation')\n",
    "    ax2.set_ylabel('Diversity Score')\n",
    "    ax2.set_title('Population Diversity Over Time')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No generation statistics available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test the Optimized System\n",
    "\n",
    "Let's test the optimized multi-hop QA system with new questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test questions\n",
    "test_questions = [\n",
    "    \"How did the discovery of DNA structure impact modern medicine?\",\n",
    "    \"What role does the Amazon rainforest play in global climate regulation?\",\n",
    "    \"How did the Industrial Revolution change social structures in Europe?\"\n",
    "]\n",
    "\n",
    "print(\"Testing Optimized Multi-Hop QA System\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, question in enumerate(test_questions):\n",
    "    print(f\"\\nQuestion {i+1}: {question}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Execute with optimized system\n",
    "    answer, trace = await qa_executor(best_qa_candidate.modules, question)\n",
    "    \n",
    "    print(f\"\\nExecution trace:\")\n",
    "    for j, module in enumerate(trace.execution_order):\n",
    "        output = trace.module_outputs.get(module, \"N/A\")[:100] + \"...\"\n",
    "        score = trace.intermediate_scores.get(module, 0)\n",
    "        print(f\"  {j+1}. {module}: {score:.3f}\")\n",
    "        print(f\"     Output: {output}\")\n",
    "    \n",
    "    print(f\"\\nFinal Score: {trace.final_score:.3f}\")\n",
    "    print(f\"Success: {trace.success}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feedback for further improvement\n",
    "feedback_extractor = StackelbergFeedbackExtractor()\n",
    "\n",
    "print(\"Module-Specific Feedback Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for module_name in qa_modules.keys():\n",
    "    if best_qa_candidate.traces:\n",
    "        feedback = feedback_extractor.extract_feedback(\n",
    "            module_name,\n",
    "            list(best_qa_candidate.traces.values()),\n",
    "            best_qa_candidate.modules[module_name]\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{module_name}:\")\n",
    "        print(f\"  Average score: {feedback['avg_score']:.3f}\")\n",
    "        print(f\"  Success rate: {feedback['success_rate']:.1%}\")\n",
    "        print(f\"  Stability: {feedback['stability']:.3f}\")\n",
    "        \n",
    "        if feedback['failure_patterns']:\n",
    "            print(f\"  Failure patterns: {', '.join(feedback['failure_patterns'][:2])}\")\n",
    "        if feedback['success_patterns']:\n",
    "            print(f\"  Success patterns: {', '.join(feedback['success_patterns'][:2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save and Export Results\n",
    "\n",
    "Save the optimized system for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export optimized prompts\n",
    "optimized_prompts = {\n",
    "    module_name: {\n",
    "        'prompt': module.prompt,\n",
    "        'type': module.module_type.value,\n",
    "        'dependencies': module.dependencies\n",
    "    }\n",
    "    for module_name, module in best_qa_candidate.modules.items()\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "import json\n",
    "with open('optimized_multihop_qa_prompts.json', 'w') as f:\n",
    "    json.dump(optimized_prompts, f, indent=2)\n",
    "\n",
    "print(\"Optimized prompts saved to 'optimized_multihop_qa_prompts.json'\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nOptimization Summary:\")\n",
    "print(f\"  Total evaluations: {qa_executor.execution_count}\")\n",
    "print(f\"  Final average score: {best_qa_candidate.get_average_score():.3f}\")\n",
    "print(f\"  Equilibrium value: {best_qa_candidate.equilibrium_value:.3f}\")\n",
    "print(f\"  Stability score: {best_qa_candidate.stability_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "In this notebook, we've:\n",
    "\n",
    "1. **Built** a sophisticated multi-hop QA system with leader-follower dynamics\n",
    "2. **Analyzed** the dependency structure and strategic interactions\n",
    "3. **Optimized** the system using Stackelberg game theory\n",
    "4. **Evaluated** the improvements in performance and stability\n",
    "5. **Tested** the optimized system on new questions\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- The **leader module** (query_generator) significantly influences downstream performance\n",
    "- **Follower modules** adapt their behavior based on leader outputs\n",
    "- **Equilibrium optimization** helps balance module interactions\n",
    "- **Stability metrics** ensure consistent performance across different inputs\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Integrate with real LLMs** (e.g., using LiteLLM)\n",
    "2. **Add more sophisticated retrieval** mechanisms\n",
    "3. **Experiment with different module configurations**\n",
    "4. **Fine-tune optimization parameters** for your use case\n",
    "5. **Extend to other multi-step reasoning tasks**\n",
    "\n",
    "For more examples and documentation, visit the [stackelberg-opt repository](https://github.com/youraanshshah/stackelberg-opt)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}